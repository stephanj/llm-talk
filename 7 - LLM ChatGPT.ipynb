{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc7e1aa-ae30-4ef2-9415-6a41bdc4f596",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai                 # OpenAI GPT\n",
    "!pip install langchain==0.0.103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76a24345-92a7-41ea-a6ef-bcd98042b04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "api_keys = dotenv_values('keys.txt')\n",
    "os.environ['OPENAI_API_KEY'] = api_keys['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "865cc175-1192-4cf9-a689-fb45ec73dcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "\n",
    "llm = OpenAI(model_name=model_name, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "211a6be6-a7c6-4607-bbf7-241207da8f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Generator\n",
    "\n",
    "def create_formatted_history(history_messages: List[dict]) -> List[Tuple[str, str]]:\n",
    "    formatted_history = []\n",
    "    user_messages = []\n",
    "    assistant_messages = []\n",
    "\n",
    "    for message in history_messages:\n",
    "        if message[\"role\"] == \"user\":\n",
    "            user_messages.append(message[\"content\"])\n",
    "        elif message[\"role\"] == \"assistant\":\n",
    "            assistant_messages.append(message[\"content\"])\n",
    "\n",
    "        if user_messages and assistant_messages:\n",
    "            formatted_history.append(\n",
    "                (\"\".join(user_messages), \"\".join(assistant_messages))\n",
    "            )\n",
    "            user_messages = []\n",
    "            assistant_messages = []\n",
    "\n",
    "    # append any remaining messages\n",
    "    if user_messages:\n",
    "        formatted_history.append((\"\".join(user_messages), None))\n",
    "    elif assistant_messages:\n",
    "        formatted_history.append((None, \"\".join(assistant_messages)))\n",
    "\n",
    "    return formatted_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6a5e2b1-7f67-4e75-ae70-64cedb1167f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"When is the Devoxx event?\"\n",
    "\n",
    "history_messages = []\n",
    "history_messages.append({\"role\": \"system\", \"content\": \"You are a conference organiser which knows everything about the event. Answer always truthfully.\"})\n",
    "\n",
    "def ask_question(question):\n",
    "    history_messages.append({\"role\": \"user\", \"content\": question})\n",
    "\n",
    "    chat_generator = llm.client.create(\n",
    "        messages = history_messages, \n",
    "        stream = True, \n",
    "        model = model_name\n",
    "    )\n",
    "\n",
    "    response_message = \"\"\n",
    "    for chunk in chat_generator:\n",
    "        if \"choices\" in chunk:\n",
    "            for choice in chunk[\"choices\"]:\n",
    "                if \"delta\" in choice and \"content\" in choice[\"delta\"]:\n",
    "                    new_token = choice[\"delta\"][\"content\"]\n",
    "                    # Add the latest token:\n",
    "                    response_message += new_token\n",
    "                    # Update the assistant's response in our model:\n",
    "                    history_messages[-1][\"content\"] = response_message\n",
    "\n",
    "                if \"finish_reason\" in choice and choice[\"finish_reason\"] == \"stop\":\n",
    "                    break\n",
    "    formatted_history = create_formatted_history(history_messages)\n",
    "\n",
    "    print(formatted_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17314651-7c99-496d-bd47-71544137872e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"I'm sorry, but I don't have access to the latest information regarding Devoxx Belgium's events. As an AI language model, my knowledge is limited to what I have been trained on. However, you could check their official website or social media pages for the most recent updates.\", None)]\n"
     ]
    }
   ],
   "source": [
    "ask_question(\"When is the next Devoxx Belgium event?\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36910863-41cc-4f93-83d6-2d252aeb9ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
